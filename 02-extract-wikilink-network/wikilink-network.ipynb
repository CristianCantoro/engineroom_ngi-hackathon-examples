{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract a network of pages from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example we query MediaWiki's [API](https://www.mediawiki.org/wiki/API:Main_page) to build a network starting from a list of \"seed\" pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the wikipedia edition we will work ok {lang}.wikipedia.org\n",
    "lang = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some options to regulate the verbosity of the output, ignore them for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "very_verbose = True\n",
    "is_new_page = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our list of seed articles is containe in `privacy_seed_keywords.txt` in the current directory.\n",
    "We can look at it with `cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internet_privacy\r\n",
      "Internet_security\r\n",
      "General_Data_Protection_Regulation\r\n",
      "Right_to_be_forgotten"
     ]
    }
   ],
   "source": [
    "! cat privacy_seed_keywords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "# the seed file is ./privacy_seed_keywords.txt\n",
    "seed_file = pathlib.Path('./privacy_seed_keywords.txt')\n",
    "\n",
    "# get the seed file without extension\n",
    "seed_filename_noext = os.path.splitext(seed_file.name)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the directory structure where we will save the results from the API, it will look like this:\n",
    "```\n",
    "data\n",
    "├── links\n",
    "│   ├── articles.txt\n",
    "│   └── ...\n",
    "└── results\n",
    "    ├── extract_network.log\n",
    "    ├── <seed_name>.csv\n",
    "    └── network_<seed_name>.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing the seed articles (./data)\n",
    "data_folder = pathlib.Path('data')\n",
    "# directory containing the outlinks of each article (./data/links)\n",
    "link_folder = data_folder/'links'\n",
    "# directory containing the resulting networks (./data/results)\n",
    "results_folder = data_folder/'results'\n",
    "\n",
    "os.makedirs(link_folder, exist_ok=True)\n",
    "os.makedirs(results_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the log file ./data/results/extract_network.log\n",
    "log_filename = 'extract_network.log'\n",
    "log_file = (results_folder/log_filename).open('a')\n",
    "\n",
    "# otpen the file containing the network ./data/results/<seed>.csv\n",
    "net_filename = 'network_' + seed_filename_noext + '.csv'\n",
    "net_file = (results_folder/net_filename).open('w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables we should get rid of\n",
    "p_id = 0\n",
    "\n",
    "first_step ={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read our seed file in a dictionary, ignoring empty lines and comments (lines starting with #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seed(file_name):\n",
    "    \"\"\"\n",
    "    Load the seed file, ignoring empty lines and comments (lines starting with #)\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    infile = pathlib.Path(file_name).open('r')\n",
    "    for line in infile:\n",
    "        el = line.strip('\\n')\n",
    "        if el and el[0] != '#':\n",
    "            el = el.replace(' ', '_')\n",
    "            dic[el] = 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Internet_privacy': 1,\n",
       " 'Internet_security': 1,\n",
       " 'General_Data_Protection_Regulation': 1,\n",
       " 'Right_to_be_forgotten': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_seed(seed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikitools import wiki\n",
    "from wikitools import api\n",
    "\n",
    "# create a Wiki object\n",
    "site = wiki.Wiki(\"https://{lang}.wikipedia.org/w/api.php\".format(lang=lang))\n",
    "\n",
    "# get outlinks of a wikipedia article through wiki api\n",
    "def get_outlinks_from_api(title):\n",
    "    p_id = -1\n",
    "    outlinks = []\n",
    "\n",
    "    if title == '' or title == ' ':\n",
    "        return p_id, outlinks\n",
    "\n",
    "    params = {'action':'query',\n",
    "              'prop':'revisions',\n",
    "              'titles': title,\n",
    "              'rvprop':'content',\n",
    "              'redirects':1\n",
    "              }\n",
    "    request = api.APIRequest(site, params)\n",
    "\n",
    "    if very_verbose:\n",
    "        print('    ' + 'query: ' + str(params))\n",
    "\n",
    "    result = request.query()\n",
    "    if int(list(result['query']['pages'].keys())[0]) < 1:\n",
    "        print('ARTICLE NOT FOUND: {}'.format(title))\n",
    "        log.write('{}\\n'.format(title))\n",
    "        return (p_id, outlinks)\n",
    "\n",
    "    else:\n",
    "        outlinks = []\n",
    "        p_id = list(result['query']['pages'].keys())[0]\n",
    "        rev = result['query']['pages'][p_id]['revisions'][0]\n",
    "        content = rev['*']\n",
    "        links = parse_text(content)\n",
    "        for l in links:\n",
    "            target = l.replace(' ', '_')\n",
    "            outlinks.append(target)\n",
    "        return (p_id, outlinks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlinks(title):\n",
    "    outlinks_filename = '{}_articles.txt'.format(title.replace('/','.'))\n",
    "    outlinks_saved = link_folder/outlinks_filename\n",
    "\n",
    "    # data saved in the disc\n",
    "    is_new_page = False\n",
    "    try:\n",
    "        with open(outlinks_saved) as f:\n",
    "            outlinks_checked = f.read().splitlines()\n",
    "    except IOError as e:\n",
    "        outlinks_checked = []\n",
    "        \n",
    "    if outlinks_checked:\n",
    "        if verbose:\n",
    "            print('{} saved links from: {}'.format(len(outlinks_checked), title))\n",
    "\n",
    "    if not outlinks_checked:\n",
    "        # get data through wiki API\n",
    "        is_new_page = True\n",
    "\n",
    "        print('title: {}'.format(title))\n",
    "        # p is the page_id\n",
    "        (p, outlinks) = get_outlinks_from_api(title)\n",
    "        \n",
    "        (redirects, outlinks_checked) = check_redirects(outlinks)\n",
    "\n",
    "    return (is_new_page,outlinks_checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_param_list(alist):\n",
    "    chunk_len = 50\n",
    "    if len(alist) < 1:\n",
    "        return ''\n",
    "    s = ['']\n",
    "    l = 0\n",
    "    i = 0\n",
    "\n",
    "    chunks = [alist[x:x+chunk_len]\n",
    "              for x in range(0, len(alist), chunk_len)\n",
    "              ]\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check redirect in Wikipedia articles/links\n",
    "def check_redirects(titles):\n",
    "\n",
    "    redirects = {}\n",
    "    links = {}\n",
    "    duplicates = 0\n",
    "\n",
    "    title_lists = split_param_list(titles)\n",
    "    for title_list in title_lists:\n",
    "\n",
    "        params = {'action':'query', 'titles':title_list, 'redirects':1} #, 'pllimit': 500, 'redirects':1}\n",
    "        request = api.APIRequest(site, params)\n",
    "        if very_verbose: print ('   ' + 'query: ' + str(params))\n",
    "        result = request.query()\n",
    "\n",
    "        if very_verbose: print(result)\n",
    "\n",
    "        if 'redirects' in result['query']:\n",
    "            for redir in result['query']['redirects']:\n",
    "                redirects[redir['from']] = redir['to']\n",
    "\n",
    "        for page in result['query']['pages']:\n",
    "            if page != '-1' and 'ns' in result['query']['pages'][page]:\n",
    "                if result['query']['pages'][page]['ns'] == 0:\n",
    "                    link = result['query']['pages'][page]['title'].replace(' ', '_')\n",
    "                    if link in links:\n",
    "                        duplicates += 1\n",
    "                    links[link] = page\n",
    "\n",
    "    missing = len(titles) - (len(links) + duplicates)\n",
    "    if very_verbose and missing != 0:\n",
    "        print('%d missing redirects (%d titles,  %d found, %d duplicates)' %(missing, len(titles), len(links), duplicates))\n",
    "    return redirects,links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regular expressions to find URLs in text\n",
    "linkSimpleP = re.compile(r'\\[\\[(.+?)[][|{}/#]')\n",
    "linkGreedyP = re.compile(r'\\[\\[([^]^[^}^{^#^/^|]+)')\n",
    "linkP = re.compile(r'\\[\\[([^]^[^}^{^#^/^]+?)\\s*(?:/[^]^[]*?)?\\s*(?:\\|[^]^[]*?)?(?:\\}\\})?\\s*\\]\\]')\n",
    "\n",
    "def parse_text(content):\n",
    "    links = {}\n",
    "    rough_links = re.findall(linkP, content) #get all links\n",
    "\n",
    "    title_lists = split_param_list(rough_links)\n",
    "\n",
    "    for title_list in title_lists:\n",
    "\n",
    "        params = {'action':'query',\n",
    "                  'titles':title_list,\n",
    "                  'redirects':1\n",
    "                  }\n",
    "        request = api.APIRequest(site, params)\n",
    "        \n",
    "        if very_verbose:\n",
    "            print ('   ' + 'query: ' + str(params))\n",
    "    \n",
    "        result = request.query()\n",
    "\n",
    "        if very_verbose:\n",
    "            print ('   ' + 'result: ' + str(result))\n",
    "\n",
    "        if result['query'].get('pages', None):\n",
    "            for page in result['query']['pages']:\n",
    "                if page != '-1' and 'ns' in result['query']['pages'][page]: # Filter just useful links. What is ns?\n",
    "                    if result['query']['pages'][page]['ns'] == 0:\n",
    "                        link = result['query']['pages'][page]['title'] #.replace(' ', '_')\n",
    "                        links[link] = 1\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import csv\n",
    "\n",
    "csv_fields = [\n",
    "'seed', \n",
    "'links_from_seed',\n",
    "'links_to_seed',\n",
    "'in_degree',\n",
    "'out_degree',\n",
    "'out_WP'\n",
    "]\n",
    "\n",
    "def extract_network(seed_file):\n",
    "    # list containing all outlinks of seed pages\n",
    "    links_to_seedlinks = []\n",
    "    # read the list of seed defined by the user\n",
    "    seeds_list = load_seed(seed_file)\n",
    "\n",
    "    for seed in set(seeds_list.keys()):\n",
    "        print(\"--> new seed article: {}\\n\".format(seed))\n",
    "\n",
    "        (is_new_page, outlinks_checked) = get_outlinks(seed) # get outlink for current seed\n",
    "        links_to_seedlinks.append(outlinks_checked) # append this outlinks to the list of all seeds' outlinks\n",
    "\n",
    "        if is_new_page: ## data not already saved in disk, save it\n",
    "            page_list_filename = seed.replace('/','.') + '_articles.txt'\n",
    "            page_list_file = (link_folder/page_list_filename).open('w')\n",
    "\n",
    "            ##########\n",
    "            # update degrees for seed and current target outlink\n",
    "            ##########\n",
    "            if seed in first_step.keys():\n",
    "                first_step[seed]['out_WP'] = len(outlinks_checked)\n",
    "\n",
    "        for target in outlinks_checked: # outlinks_checked contains the outlinks of the current keyword\n",
    "            if seed != target:\n",
    "                net_file.write( seed + '\\t' + target  + '\\n') # write edge as pair of nodes (source, target)\n",
    "                if is_new_page:\n",
    "                    page_list_file.write(target + '\\n')\n",
    "                try:\n",
    "                    #### 17.01\n",
    "                    first_step[seed]['out_degree'] += 1\n",
    "                    #print('seed outdegree ', seed.decode('utf8'), first_step[seed.decode('utf8')]['out_degree'])\n",
    "                    if(target in seeds_list):\n",
    "                        first_step[seed]['links_to_seed'] += 1\n",
    "                    ####\n",
    "                except:\n",
    "                    if(target in seeds_list):\n",
    "                        first_step[seed] = {'seed': True,\n",
    "                                            'links_from_seed': 0,\n",
    "                                            'links_to_seed': 1,\n",
    "                                            'in_degree': 0,\n",
    "                                            'out_degree': 1,\n",
    "                                            'out_WP': len(outlinks_checked)\n",
    "                                            }\n",
    "                    else:\n",
    "                        first_step[seed] = {'seed': True,\n",
    "                                            'links_from_seed': 0,\n",
    "                                            'links_to_seed': 0,\n",
    "                                            'in_degree': 0,\n",
    "                                            'out_degree': 1,\n",
    "                                            'out_WP': len(outlinks_checked)\n",
    "                                            }\n",
    "                try:\n",
    "                    first_step[target]['links_from_seed'] += 1\n",
    "                    first_step[target]['in_degree'] += 1\n",
    "                    if(target in seeds_list):\n",
    "                        first_step[target]['seed'] = True\n",
    "                except:\n",
    "                    if(target in seeds_list):\n",
    "                        first_step[target] = {'seed': True,\n",
    "                                              'links_from_seed': 1,\n",
    "                                              'links_to_seed': 0,\n",
    "                                              'in_degree': 1,\n",
    "                                              'out_degree': 0,\n",
    "                                              'out_WP': 0\n",
    "                                              }\n",
    "                    else:\n",
    "                        first_step[target] = {'seed': False,\n",
    "                                              'links_from_seed': 1,\n",
    "                                              'links_to_seed': 0,\n",
    "                                              'in_degree': 1,\n",
    "                                              'out_degree': 0,\n",
    "                                              'out_WP': 0\n",
    "                                              }\n",
    "\n",
    "        if is_new_page:\n",
    "            page_list_file.close()\n",
    "\n",
    "    #from the list of outlinks keep only those that are no seeds and remove duplicates\n",
    "    links_to_seedlinks = list(itertools.chain.from_iterable(links_to_seedlinks))\n",
    "    links_to_seedlinks = list(set(links_to_seedlinks) - set(seeds_list.keys()) )\n",
    "\n",
    "    ##########\n",
    "    # Start new iteraction over outlinks of the seed articles\n",
    "    ##########\n",
    "    olink_index = 1 #just a counter\n",
    "    for title in links_to_seedlinks:\n",
    "        print(\"\\n\\n outlink \" + str(olink_index) + \" of: \" + str(len(links_to_seedlinks)) + \"\\n\\n\")\n",
    "        olink_index += 1\n",
    "        (is_new_page, outlinks_checked) = get_outlinks(title) # get outlinks of current articles\n",
    "\n",
    "        if is_new_page: ## data not already saved in disk, save it\n",
    "            page_list_file_title_name = title.replace('/','.') + '_articles.txt'\n",
    "            page_list_file_title = (link_folder/page_list_file_title_name).open('w')\n",
    "\n",
    "            ##########\n",
    "            # update degress of source and target\n",
    "            ##########\n",
    "            first_step[title]['out_WP'] = len(outlinks_checked)\n",
    "        for target in outlinks_checked: # outlinks_checked contains the outlinks of the current keyword\n",
    "            if is_new_page:\n",
    "                page_list_file_title.write(target + '\\n')\n",
    "            if target in seeds_list.keys( ) and target != title:\n",
    "                net_file.write( title + '\\t' + target  + '\\n') # write edge as pair of nodes\n",
    "                first_step[title]['links_to_seed'] += 1\n",
    "                first_step[title]['out_degree'] += 1\n",
    "                first_step[target]['in_degree'] += 1\n",
    "            else:\n",
    "                if target in links_to_seedlinks and target != title:\n",
    "                    net_file.write( title + '\\t' + target  + '\\n') # write edge as pair of nodes\n",
    "                    first_step[title]['out_degree'] += 1\n",
    "                    first_step[target]['in_degree'] += 1\n",
    "        # note that I write inside each if because I write only if the target is either a seed or a outlink of a seed (since we\n",
    "        # only want the first step degree net)\n",
    "        ##########\n",
    "        ##########\n",
    "        if is_new_page:\n",
    "            page_list_file_title.close()\n",
    "\n",
    "    net_file.close()\n",
    "    # Write degrees in a file\n",
    "    output_filename = 'first_step_degrees_{}.csv'.format(seed_filename_noext)\n",
    "    with (results_folder/output_filename).open('w') as outfile:\n",
    "        writer = csv.writer(outfile, delimiter =\"\\t\")\n",
    "        writer.writerow(['Page'] + csv_fields)\n",
    "        for page in first_step.keys():\n",
    "            writer.writerow([page] + [first_step[page][degree] for degree in csv_fields])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> new seed article: Internet_privacy\n",
      "\n",
      "1 saved links from: Internet_privacy\n",
      "--> new seed article: Internet_security\n",
      "\n",
      "1 saved links from: Internet_security\n",
      "--> new seed article: General_Data_Protection_Regulation\n",
      "\n",
      "1 saved links from: General_Data_Protection_Regulation\n",
      "--> new seed article: Right_to_be_forgotten\n",
      "\n",
      "1 saved links from: Right_to_be_forgotten\n",
      "\n",
      "\n",
      " outlink 1 of: 4\n",
      "\n",
      "\n",
      "1 saved links from: Streisand_effect\n",
      "\n",
      "\n",
      " outlink 2 of: 4\n",
      "\n",
      "\n",
      "1 saved links from: Jeff_Flake\n",
      "\n",
      "\n",
      " outlink 3 of: 4\n",
      "\n",
      "\n",
      "title: Digital_economy\n",
      "    query: {'action': 'query', 'prop': 'revisions', 'titles': 'Digital_economy', 'rvprop': 'content', 'redirects': 1}\n",
      "   query: {'action': 'query', 'titles': ['internet', 'World Wide Web', 'New Economy', 'Don Tapscott', 'Thomas Mesenbourg', 'social media', 'Nicholas Negroponte', 'Oxford Economics', 'Deloitte Digital', 'Telstra', 'The Boston Consulting Group', 'bcg.perspectives', 'Deloitte', 'Econsultancy', 'National Broadband Network', 'Bitcoin', 'Digitization economics', 'Electronic business', 'Electronic commerce', 'Indigo Era (economics)', 'Industry 4.0', 'Information economy', 'information society', 'Knowledge economy', 'Knowledge management', 'Knowledge market', 'National Broadband Network', 'Network economy', 'Virtual economy', 'Monthly Review', 'Category:Cashless society', 'Category:Economic systems', 'Category:Information economy'], 'redirects': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristian/Dottorato/Works/ngi/engineroom_ngi-hackathon-examples/venvs/ngi-ds3/lib/python3.7/site-packages/wikitools/api.py:108: FutureWarning: The querycontinue option is deprecated and will be removed\n",
      "in a future release, use the new queryGen function instead\n",
      "for queries requring multiple requests\n",
      "  for queries requring multiple requests\"\"\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   result: {'batchcomplete': '', 'query': {'pages': {'26952279': {'pageid': 26952279, 'ns': 14, 'title': 'Category:Information economy'}}}}\n",
      "\n",
      "\n",
      " outlink 4 of: 4\n",
      "\n",
      "\n",
      "title: Internet_safety\n",
      "    query: {'action': 'query', 'prop': 'revisions', 'titles': 'Internet_safety', 'rvprop': 'content', 'redirects': 1}\n",
      "   query: {'action': 'query', 'titles': ['Information privacy', 'internet', 'computer crime', 'United Kingdom', 'Serious Organized Crime Agency', 'Microsoft', 'eBay', 'Personally identifiable information', 'identity theft', 'Phishing', 'Internet Fraud', 'Malware', 'spyware', 'Computer-mediated communication', 'Cyberbullying', 'Internet-initiated sex crimes against minors', 'chat room', 'internet forum', 'internet', 'shock sites', 'hate speech', 'pop-up ad', 'Sextortion', 'webcam', 'flirting', 'cybersex', 'Cybercrime', 'Social engineering (security)', 'Physical attractiveness', 'masturbation', 'Internet-initiated sex crimes against minors', 'National Crime Agency', 'United Kingdom', 'humiliation', 'Suicide', 'Accountability software', 'Content control software', 'Identity fraud', 'Internet crime', 'Internet fraud', 'Internet security', 'Website reputation ratings', 'AHTCC', 'Childnet', 'Incredible Internet', 'Sonia Livingstone', 'ThinkUKnow', 'Tween summit', 'Youth Internet Safety Survey', 'Category:Crime prevention'], 'redirects': 1}\n",
      "   result: {'batchcomplete': '', 'query': {'pages': {'3541636': {'pageid': 3541636, 'ns': 14, 'title': 'Category:Crime prevention'}}}}\n",
      "   query: {'action': 'query', 'titles': ['Category:Internet safety'], 'redirects': 1}\n",
      "   result: {'batchcomplete': '', 'query': {'pages': {'19571188': {'pageid': 19571188, 'ns': 14, 'title': 'Category:Internet safety'}}}}\n"
     ]
    }
   ],
   "source": [
    "extract_network(seed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links  results\r\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
